name: Comprehensive Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false # Don't cancel other jobs if one fails
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install nox

      - name: Run CI pipeline
        run: nox -s ci --python ${{ matrix.python-version }}

      - name: Upload coverage to Codecov
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml

  reproducibility:
    runs-on: ubuntu-latest
    needs: test

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,benchmark]"

      - name: Test reproducibility
        run: |
          # Check if test data exists
          if [ -f "data/input/gotofiles/2.tsv" ]; then
            echo "Running reproducibility test with available data..."
            # Run Phase 2 twice and compare outputs using tracked test data
            python src/main.py data/input/gotofiles/2.tsv -o run1.csv
            python src/main.py data/input/gotofiles/2.tsv -o run2.csv

            # Check if outputs are identical
            if ! diff -q run1.csv run2.csv > /dev/null; then
              echo "ERROR: Phase 2 outputs are not reproducible"
              echo "Differences found:"
              diff run1.csv run2.csv || true
              exit 1
            else
              echo "SUCCESS: Phase 2 outputs are reproducible"
            fi
          else
            echo "⚠️ Test data not available, skipping reproducibility test"
            echo "✅ Reproducibility test skipped (no test data)"
          fi

      - name: Performance benchmarking
        run: |
          # memory-profiler and psutil are already installed via benchmark dependencies
          # Add basic performance tests using tracked test data
          python -c "
          import os
          import time
          import subprocess

          if os.path.exists('data/input/gotofiles/2.tsv'):
              print('Running performance benchmark with available data...')
              start = time.time()
              result = subprocess.run(['python', 'src/main.py', 'data/input/gotofiles/2.tsv', '-o', 'benchmark.csv'], capture_output=True)
              end = time.time()
              print(f'Phase 2 execution time: {end-start:.2f} seconds')
              if end-start > 30:  # Fail if takes more than 30 seconds
                  print('⚠️ Performance test failed: execution time exceeded 30 seconds')
                  exit(1)
              else:
                  print('✅ Performance benchmark completed successfully')
          else:
              print('⚠️ Test data not available, skipping performance benchmark')
              print('✅ Performance benchmark skipped (no test data)')
          "
